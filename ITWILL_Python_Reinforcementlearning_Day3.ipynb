{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 시간차 학습 p.105\n",
    "\n",
    "몬테카를로의 단점을 개선을 한것이 시간차 학습 입니다,  \n",
    "__몬테카를로 알고리즘의 단점__은, 바둑의 경우처럼 에피소드의 길이가 길거나 에피소드의 끝이 없는 주식시장처럼 에피소드의 끝이 없는경우는   \n",
    "몬테카를로 예측이 적합하지 않습니다.  \n",
    "\n",
    "### off policy는 말 그대로 현재 행동하는 정책과는 독립적으로 학습한다는 것입니다.  \n",
    "### 즉 행동하는 정책과 학습하는 정책을  따로 분리하는것을 말합니다.  \n",
    "### off policy를 적용한게 큐러닝인데 1989년 chris watkin라는 분에 의해서 소개가 되었습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 큐러닝(p.115)\n",
    "\n",
    "벨만 기대 방정식 --> 정책 반복 --> 살사\n",
    "벨만 최적 방정식 --> 가치 반복 --> 큐러닝(off policy)\n",
    "\n",
    "__첨부파일__: Qlearning.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제.(오늘의 마지막 문제) 큐러닝 틱텍토를 살사 틱텍토로 변경하시오~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 인공지능 틱텍토 프로그램의 큰 그림( p.249)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enviroment 클래스 (p.255)\n",
    "\n",
    "- init 함수 : 게임 초기화\n",
    "- move 함수 : 플레이어의 선택을 보드에 표시하고 게임이 종료되었는지를 체크\n",
    "- get_action 함수 : 현재 보드 상태에서 가능한 행동(둘수있는 장소) 선택\n",
    "- end_check 함수 : 게임이 종료되었는지 판단\n",
    "- print_board 함수 : 현재 보드의 상태를 출력\n",
    "\n",
    "## 2. 인간 플레이어 클래스\n",
    "## 3. 랜덤 플레이어 클래스\n",
    "## 4. 몬테카를로 플레이어 클래스\n",
    "## 5. Q-learning 플레이어 클래스\n",
    "## 6. DQN 플레이어 클래스(신경망)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 큐러닝 플레이어 클래스를 이해하기 위해서 알아야 할 파이썬 기본 문법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 1. np.random.choice\n",
    "\n",
    "import  numpy as  np\n",
    "\n",
    "# pr의 리스트는 합쳐서 무조건 1이 되어야 한다.\n",
    "pr= [0.1,0.1,0.8]\n",
    "\n",
    "action = np.random.choice(range(0,3), p=pr)  # [0,1,2]\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 30.0\n",
      "\n",
      "\n",
      "[0.1 0.2]\n",
      "[1.  2.  3.  0.  0.  0.  0.1 0.2]\n"
     ]
    }
   ],
   "source": [
    "# 2. np,where\n",
    "\n",
    "import numpy as np\n",
    "a = np.array([1,2,3,10,20,30,0.1,0.2])\n",
    "\n",
    "print(np.min(a), np.max(a))\n",
    "print('\\n')\n",
    "print(a[np.where(a<1)])\n",
    "print(np.where(a>=10,0,a)) # a numpy 리스트에서 10보다 크거나 같으면 0으로 아니면 a로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.234, 'b': 0.443}\n",
      "dict_keys(['a', 'b'])\n",
      "dict_values([0.234, 0.443])\n",
      "dict_items([('a', 0.234), ('b', 0.443)])\n",
      "['a', 'b']\n"
     ]
    }
   ],
   "source": [
    "# 3. 딕셔너리 자료형 만들기\n",
    "\n",
    "dic = {}\n",
    "dic['a'] = 0.234\n",
    "dic['b'] = 0.443\n",
    "\n",
    "print(dic)\n",
    "print(dic.keys())\n",
    "print(dic.values())\n",
    "print(dic.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 틱텍토의 학습데이터가 쌓이는 딕셔너리의 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{((1.0, 1.0, 0.0, 0.0, -1.0, -1.0, -1.0, 0.0, 1.0), 3): 0}\n"
     ]
    }
   ],
   "source": [
    "qtable = {}\n",
    "key = ((1.0, 1.0, 0.0, 0.0, -1.0, -1.0, -1.0, 0.0, 1.0), 3) # 앞의것은 보드판 상황, 뒤의 3은 3번자리에 둘 것이라는 action\n",
    "qtable[key] = 0 # 3번자리에 두는것에 대한 가치 (초기 가치라서 0으로 설정)\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.0, 1.0, 0.0,   \n",
    "0.0, -1.0, -1.0,   \n",
    "-1.0, 0.0, 1.0)  \n",
    "\n",
    "| o | o |   |\n",
    "|:-:|:-:|---|\n",
    "|   | x | x |\n",
    "| x |   | o |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:51<00:00,  9.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# 4. tqdm 모듈 사용법\n",
    "# 루프문의 진행상황을 막대그래프로 표시해줌.\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "for step in tqdm(range(500)):\n",
    "    sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ Q-leanring 클래스 리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{((1.0, 1.0, 0.0, 0.0, -1.0, -1.0, -1.0, 0.0, 1.0), 3): 0}\n"
     ]
    }
   ],
   "source": [
    "# init 참고 코드\n",
    "\n",
    "qtable = {}\n",
    "key = ((1.0, 1.0, 0.0, 0.0, -1.0, -1.0, -1.0, 0.0, 1.0), 3) # 앞의것은 보드판 상황, 뒤의 3은 3번자리에 둘 것이라는 action\n",
    "qtable[key] = 0 # 3번자리에 두는것에 대한 가치 (초기 가치라서 0으로 설정)\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Q_player\"\n",
    "        # Q-table을 딕셔너리로 정의\n",
    "        self.qtable = {} # ((1.0, 1.0, 0.0, 0.0, -1.0, -1.0, -1.0, 0.0, 1.0), 3) = 0.2912\n",
    "        # e-greedy 계수 정의\n",
    "        self.epsilon = 1 # 학습할때 epsilon을 0.5로 하고 실제로 사람과 게임할 때는 랜덤수를 두지 않게 한다.\n",
    "        \n",
    "        # 학습률 정의\n",
    "        self.learning_rate = 0.1\n",
    "        self.gamma=0.9 # 감가율\n",
    "        self.print = False\n",
    "\n",
    "    # policy에 따라 상태에 맞는 행동을 선택\n",
    "    def select_action(self, env, player):\n",
    "        # policy에 따라 행동을 선택\n",
    "        action = self.policy(env) # env : 보드판\n",
    "    \n",
    "        return action # 0,1,2,3,4,5,6,7,8 중에 하나가 출력\n",
    "        \n",
    "    def policy(self, env):\n",
    "        # 행동 가능한 상태를 저장\n",
    "        available_action = env.get_action() # [2,3,4]\n",
    "        # 행동 가능한 상태의 Q-value를 저장\n",
    "        qvalues = np.zeros(len(available_action)) # 가치값(Q값) [0,0,0]\n",
    "        \n",
    "\n",
    "        if self.print:\n",
    "            print(\"{} : available_action\".format(available_action))\n",
    "\n",
    "        # 행동 가능한 상태의 Q-value를 조사\n",
    "        for i, act in enumerate(available_action): # [2.3.4]\n",
    "\n",
    "            key = (tuple(env.board_a),act)\n",
    "            # ((1.0, 1.0, 0.0, 0.0, -1.0, -1.0, -1.0, 0.0, 1.0), 3) \n",
    "\n",
    "            # 현재 상태를 경험한 적이 없다면(딕셔너리에 없다면) 딕셔너리에 추가(Q-value = 0)\n",
    "            if self.qtable.get(key) ==  None:                \n",
    "                self.qtable[key] = 0\n",
    "                \n",
    "            # 행동 가능한 상태의 Q-value 저장 (현재상태를 경험한 적이 있을 경우)\n",
    "            qvalues[i] = self.qtable.get(key)\n",
    "     \n",
    "\n",
    "\n",
    "        # e-greedy\n",
    "        # 가능한 행동들 중에서 Q-value가 가장 큰 행동을 저장\n",
    "        greedy_action = np.argmax(qvalues)                    \n",
    "        \n",
    "        pr = np.zeros(len(available_action)) # [0,0,0]\n",
    "      \n",
    "\n",
    "        # max Q-value와 같은 값이 여러개 있는지 확인한 후 double_check에 상태를 저장\n",
    "        double_check = (np.where(qvalues == np.max(qvalues),1,0))\n",
    "\n",
    "        #  여러개 있다면 중복된 상태중에서 다시 무작위로 선택    \n",
    "        if np.sum(double_check) > 1:\n",
    "         \n",
    "            double_check = double_check/np.sum(double_check)\n",
    "            greedy_action =  np.random.choice(range(0,len(double_check)), p=double_check)\n",
    "            \n",
    "        # e-greedy로 행동들의 선택 확률을 계산\n",
    "        pr = np.zeros(len(available_action))\n",
    "        \n",
    "    # epsilon 이 0.5이므로 두번중에 한번은 경험이고 한번은 random이다.\n",
    "        for i in range(len(available_action)):\n",
    "            if i == greedy_action:\n",
    "                pr[i] = 1 - self.epsilon + self.epsilon/len(available_action)\n",
    "                if pr[i] < 0:\n",
    "                    print(\"{} : - pr\".format(np.round(pr[i],2)))\n",
    "            else:\n",
    "                pr[i] = self.epsilon / len(available_action)\n",
    "                if pr[i] < 0:\n",
    "                    print(\"{} : - pr\".format(np.round(pr[i],2)))\n",
    "\n",
    "        action = np.random.choice(range(0,len(available_action)), p=pr)\n",
    "\n",
    "      \n",
    "        return available_action[action]    # 3,2 등등 random\n",
    "    \n",
    "    def learn_qtable(self,board_bakup, action_backup, env, reward):\n",
    "        # 현재 상태와 행동을 키로 저장\n",
    "        key = (board_bakup,action_backup)\n",
    "     \n",
    "        # Q-table 학습\n",
    "        if env.done == True:\n",
    "            # 게임이 끝났을 경우 학습\n",
    "\n",
    "            self.qtable[key] += self.learning_rate*(reward - self.qtable[key])\n",
    "            \n",
    "         \n",
    "        else:\n",
    "            # 게임이 진행중일 경우 학습\n",
    "            # 다음 상태의 max Q 값 계산\n",
    "            available_action = env.get_action()        \n",
    "            qvalues = np.zeros(len(available_action))\n",
    "\n",
    "            for i, act in enumerate(available_action):\n",
    "                next_key = (tuple(env.board_a),act)\n",
    "                # 다음 상태를 경험한 적이 없다면(딕셔너리에 없다면) 딕셔너리에 추가(Q-value = 0)\n",
    "                if self.qtable.get(next_key) ==  None:                \n",
    "                    self.qtable[next_key] = 0\n",
    "                qvalues[i] = self.qtable.get(next_key)\n",
    "\n",
    "            # maxQ 조사\n",
    "            maxQ = np.max(qvalues)  \n",
    "            \n",
    "            # 게임이 진행중일 때 학습\n",
    "            self.qtable[key] += self.learning_rate*(reward + self.gamma * maxQ - self.qtable[key])\n",
    "        \n",
    "                \n",
    "        if self.print:\n",
    "            print(\"-----------   learn_qtable end -------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
